{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: getting and transforming the data\n",
    "### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the data\n",
    "df = pd.read_csv('sensor.csv', parse_dates=['timestamp'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and testing and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the dataset for training, validation, and testing\n",
    "df_train = df[df['timestamp'] < '2018-07-01']\n",
    "df_valid = df[(df['timestamp'] >= '2018-07-01') & (df['timestamp'] < '2018-08-01')]\n",
    "df_test = df[df['timestamp'] >= '2018-08-01']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save these three parts to separate CSV files:\n",
    "df_train.to_csv('train.csv', index=False)\n",
    "df_valid.to_csv('valid.csv', index=False)\n",
    "df_test.to_csv('test.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: create the model and the drawer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given this is anomaly detection on time-series data, one possible method is to use an Isolation Forest. This is an unsupervised learning algorithm that works well for anomaly detection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "df_train = pd.read_csv('train.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# Drop the columns we won't use\n",
    "df_train.drop(['Unnamed: 0', 'timestamp', 'machine_status'], axis=1, inplace=True)\n",
    "\n",
    "# Fill any NaN values with the mean\n",
    "df_train.fillna(df_train.mean(), inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data-transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'sensor_15' column\n",
    "df_train.drop('sensor_15', axis=1, inplace=True)\n",
    "\n",
    "# Fill any NaN values with the mean\n",
    "df_train.fillna(df_train.mean(), inplace=True)\n",
    "\n",
    "# Then, check again if any NaN values still exist\n",
    "print(df_train.isna().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df_train), columns=df_train.columns)\n",
    "\n",
    "# Define the model\n",
    "model = IsolationForest(contamination=0.05)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(df_scaled)\n",
    "\n",
    "# Apply the trained model to the data\n",
    "scores = model.decision_function(df_scaled)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  save the model and the scaler for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model and the scaler\n",
    "joblib.dump(model, 'model.joblib')\n",
    "joblib.dump(scaler, 'scaler.joblib')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class DataHandler(FileSystemEventHandler):\n",
    "    '''Handles new data files as they appear in the input directory.'''\n",
    "\n",
    "    def __init__(self, config):\n",
    "        '''Initializes the DataHandler with the given configuration.'''\n",
    "    \n",
    "        self.config = config\n",
    "        self.model = joblib.load(config['model_path'])\n",
    "        self.scaler = joblib.load(config['scaler_path'])\n",
    "        self.setup_logging()\n",
    "\n",
    "    def setup_logging(self):\n",
    "        '''Sets up logging to write to the log file.'''\n",
    "        logging.basicConfig(filename=self.config['log_path'], level=logging.INFO)\n",
    "        logging.info('Application started.')\n",
    "\n",
    "    def on_created(self, event):\n",
    "        '''Called when a new file is created in the input directory.'''\n",
    "        filename = event.src_path\n",
    "        logging.info(f'New file detected: {filename}')\n",
    "\n",
    "        try:\n",
    "            data = pd.read_csv(filename)\n",
    "            transformed_data = self.transform_data(data)\n",
    "            predictions = self.model.predict(transformed_data)\n",
    "            enriched_predictions = transformed_data.copy()\n",
    "            enriched_predictions['prediction'] = predictions\n",
    "            enriched_predictions.to_csv(os.path.join(self.config['output_directory'], os.path.basename(filename)))\n",
    "\n",
    "            for sensor in self.config['sensors_to_draw']:\n",
    "                self.plot_sensor(enriched_predictions, sensor)\n",
    "\n",
    "            logging.info('Processing complete.')\n",
    "        except Exception as e:\n",
    "            logging.error(f'Error processing file: {e}')\n",
    "\n",
    "    def transform_data(self, data):\n",
    "        '''Transforms the data into a format that can be used by the model.'''\n",
    "        \n",
    "        # Apply transformations here. This is just an example.\n",
    "        data = data.fillna(data.mean())\n",
    "        data = pd.DataFrame(self.scaler.transform(data), columns=data.columns)\n",
    "        return data\n",
    "\n",
    "    def plot_sensor(self, data, sensor):\n",
    "        '''Plots the given sensor data and saves it to a file.'''\n",
    "        fig, ax = plt.subplots()\n",
    "        data[sensor].plot(ax=ax)\n",
    "        fig.savefig(os.path.join(self.config['image_directory'], f'{sensor}.png'))\n",
    "\n",
    "\n",
    "def load_config(config_file):\n",
    "    '''Loads the configuration from the given JSON file.'''\n",
    "    with open(config_file) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def main():\n",
    "    config = load_config('config.json')\n",
    "    event_handler = DataHandler(config)\n",
    "\n",
    "    observer = Observer()\n",
    "    observer.schedule(event_handler, config['input_directory'], recursive=False)\n",
    "    observer.start()\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        observer.stop()\n",
    "\n",
    "    observer.join()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
