{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: getting and transforming the data\n",
    "### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the data\n",
    "df = pd.read_csv('sensor.csv', parse_dates=['timestamp'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and testing and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the dataset for training, validation, and testing\n",
    "df_train = df[df['timestamp'] < '2018-07-01']\n",
    "df_valid = df[(df['timestamp'] >= '2018-07-01') & (df['timestamp'] < '2018-08-01')]\n",
    "df_test = df[df['timestamp'] >= '2018-08-01']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save these three parts to separate CSV files:\n",
    "df_train.to_csv('train.csv', index=False)\n",
    "df_valid.to_csv('valid.csv', index=False)\n",
    "df_test.to_csv('test.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: create the model and the drawer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given this is anomaly detection on time-series data, one possible method is to use an Isolation Forest. This is an unsupervised learning algorithm that works well for anomaly detection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "df_train = pd.read_csv('train.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# Drop the columns we won't use\n",
    "df_train.drop(['Unnamed: 0', 'timestamp', 'machine_status'], axis=1, inplace=True)\n",
    "\n",
    "# Fill any NaN values with the mean\n",
    "df_train.fillna(df_train.mean(), inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data-transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensor_00         0.167686\n",
      "sensor_01         6.860247\n",
      "sensor_02        10.313073\n",
      "sensor_03         4.790412\n",
      "sensor_04     19691.077418\n",
      "sensor_05       365.869523\n",
      "sensor_06         4.568557\n",
      "sensor_07         5.175857\n",
      "sensor_08         4.069469\n",
      "sensor_09         4.671920\n",
      "sensor_10       146.821045\n",
      "sensor_11       146.540876\n",
      "sensor_12       100.854835\n",
      "sensor_13        20.465962\n",
      "sensor_14     19273.190478\n",
      "sensor_15              NaN\n",
      "sensor_16     24060.658960\n",
      "sensor_17     25932.878453\n",
      "sensor_18         0.905067\n",
      "sensor_19     59759.872045\n",
      "sensor_20     15597.601355\n",
      "sensor_21     77427.332346\n",
      "sensor_22     33563.133204\n",
      "sensor_23    109951.805812\n",
      "sensor_24     49951.411656\n",
      "sensor_25     72442.435995\n",
      "sensor_26     81863.037311\n",
      "sensor_27     29013.023414\n",
      "sensor_28    139018.796364\n",
      "sensor_29     82985.104657\n",
      "sensor_30     53066.943340\n",
      "sensor_31    122393.236814\n",
      "sensor_32     88870.048974\n",
      "sensor_33     36159.822098\n",
      "sensor_34      7439.922658\n",
      "sensor_35     19362.868357\n",
      "sensor_36     95662.522637\n",
      "sensor_37       742.285218\n",
      "sensor_38       125.014929\n",
      "sensor_39       312.312628\n",
      "sensor_40       416.949376\n",
      "sensor_41        76.515947\n",
      "sensor_42       150.721831\n",
      "sensor_43       154.949140\n",
      "sensor_44        50.040738\n",
      "sensor_45       104.006531\n",
      "sensor_46       178.593307\n",
      "sensor_47        66.416238\n",
      "sensor_48      3261.946670\n",
      "sensor_49       181.631051\n",
      "sensor_50      3249.137248\n",
      "sensor_51      5839.298389\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df_train.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensor_00         0\n",
      "sensor_01         0\n",
      "sensor_02         0\n",
      "sensor_03         0\n",
      "sensor_04         0\n",
      "sensor_05         0\n",
      "sensor_06         0\n",
      "sensor_07         0\n",
      "sensor_08         0\n",
      "sensor_09         0\n",
      "sensor_10         0\n",
      "sensor_11         0\n",
      "sensor_12         0\n",
      "sensor_13         0\n",
      "sensor_14         0\n",
      "sensor_15    131040\n",
      "sensor_16         0\n",
      "sensor_17         0\n",
      "sensor_18         0\n",
      "sensor_19         0\n",
      "sensor_20         0\n",
      "sensor_21         0\n",
      "sensor_22         0\n",
      "sensor_23         0\n",
      "sensor_24         0\n",
      "sensor_25         0\n",
      "sensor_26         0\n",
      "sensor_27         0\n",
      "sensor_28         0\n",
      "sensor_29         0\n",
      "sensor_30         0\n",
      "sensor_31         0\n",
      "sensor_32         0\n",
      "sensor_33         0\n",
      "sensor_34         0\n",
      "sensor_35         0\n",
      "sensor_36         0\n",
      "sensor_37         0\n",
      "sensor_38         0\n",
      "sensor_39         0\n",
      "sensor_40         0\n",
      "sensor_41         0\n",
      "sensor_42         0\n",
      "sensor_43         0\n",
      "sensor_44         0\n",
      "sensor_45         0\n",
      "sensor_46         0\n",
      "sensor_47         0\n",
      "sensor_48         0\n",
      "sensor_49         0\n",
      "sensor_50         0\n",
      "sensor_51         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensor_00    0\n",
      "sensor_01    0\n",
      "sensor_02    0\n",
      "sensor_03    0\n",
      "sensor_04    0\n",
      "sensor_05    0\n",
      "sensor_06    0\n",
      "sensor_07    0\n",
      "sensor_08    0\n",
      "sensor_09    0\n",
      "sensor_10    0\n",
      "sensor_11    0\n",
      "sensor_12    0\n",
      "sensor_13    0\n",
      "sensor_14    0\n",
      "sensor_16    0\n",
      "sensor_17    0\n",
      "sensor_18    0\n",
      "sensor_19    0\n",
      "sensor_20    0\n",
      "sensor_21    0\n",
      "sensor_22    0\n",
      "sensor_23    0\n",
      "sensor_24    0\n",
      "sensor_25    0\n",
      "sensor_26    0\n",
      "sensor_27    0\n",
      "sensor_28    0\n",
      "sensor_29    0\n",
      "sensor_30    0\n",
      "sensor_31    0\n",
      "sensor_32    0\n",
      "sensor_33    0\n",
      "sensor_34    0\n",
      "sensor_35    0\n",
      "sensor_36    0\n",
      "sensor_37    0\n",
      "sensor_38    0\n",
      "sensor_39    0\n",
      "sensor_40    0\n",
      "sensor_41    0\n",
      "sensor_42    0\n",
      "sensor_43    0\n",
      "sensor_44    0\n",
      "sensor_45    0\n",
      "sensor_46    0\n",
      "sensor_47    0\n",
      "sensor_48    0\n",
      "sensor_49    0\n",
      "sensor_50    0\n",
      "sensor_51    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop 'sensor_15' column\n",
    "df_train.drop('sensor_15', axis=1, inplace=True)\n",
    "\n",
    "# Fill any NaN values with the mean\n",
    "df_train.fillna(df_train.mean(), inplace=True)\n",
    "\n",
    "# Then, check again if any NaN values still exist\n",
    "print(df_train.isna().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df_train), columns=df_train.columns)\n",
    "\n",
    "# Define the model\n",
    "model = IsolationForest(contamination=0.05)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(df_scaled)\n",
    "\n",
    "# Apply the trained model to the data\n",
    "scores = model.decision_function(df_scaled)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  save the model and the scaler for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.joblib']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model and the scaler\n",
    "joblib.dump(model, 'model.joblib')\n",
    "joblib.dump(scaler, 'scaler.joblib')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
