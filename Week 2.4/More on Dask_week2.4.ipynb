{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'.\\test\\groupby-N_10000000_K_100_file_0.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create a timing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "def perform_test(df):\n",
    "    ''' Perform the desired operation on the DataFrame\n",
    "    and return the execution time'''\n",
    "    \n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        # Perform operation using Pandas\n",
    "        start_time = time.time()\n",
    "        # Perform the desired operation on the Pandas DataFrame\n",
    "        # For example, grouping the DataFrame by 'id1' and summing 'v1'\n",
    "        result = df.groupby('id1')['v1'].sum()\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "    elif isinstance(df, dd.DataFrame):\n",
    "        # Perform operation using Dask\n",
    "        start_time = time.time()\n",
    "        # Perform the desired operation on the Dask DataFrame\n",
    "        # For example, grouping the DataFrame by 'id1' and summing 'v1'\n",
    "        result = df.groupby('id1')['v1'].sum().compute()\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "\n",
    "    return execution_time\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function checks the type of the input DataFrame (Pandas or Dask) using isinstance and performs the operation accordingly. For Dask DataFrames, the compute() method is used to trigger the computation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizing the performance of a Dask DataFrame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setting a Dask baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "dtypes = {\n",
    "    \"id1\": \"object\",\n",
    "    \"id2\": \"object\",\n",
    "    \"id3\": \"object\",\n",
    "    \"id4\": \"object\",\n",
    "    \"id5\": \"object\",\n",
    "    \"id6\": \"object\",\n",
    "    \"v1\": \"int64\",\n",
    "    \"v2\": \"object\",\n",
    "    \"v3\": \"object\",\n",
    "}\n",
    "\n",
    "df_dask = dd.read_csv(r'.\\test\\groupby-N_10000000_K_100_file_0.csv', dtype=dtypes)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Avoiding object columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the data types of columns 'id1', 'id2', and 'id3' to string[pyarrow] type, and 'v3' to float64. The rest of the columns can be considered as int64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "dtypes = {\n",
    "    \"id1\": \"string[pyarrow]\",\n",
    "    \"id2\": \"string[pyarrow]\",\n",
    "    \"id3\": \"string[pyarrow]\",\n",
    "    \"id4\": \"int64\",\n",
    "    \"id5\": \"int64\",\n",
    "    \"id6\": \"int64\",\n",
    "    \"v1\": \"int64\",\n",
    "    \"v2\": \"int64\",\n",
    "    \"v3\": \"float64\",\n",
    "}\n",
    "\n",
    "df_dask = dd.read_csv(r'.\\test\\groupby-N_10000000_K_100_file_0.csv', dtype=dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Repartition the data into multiple files\n",
    "df_dask = df_dask.repartition(partition_size='100MB')\n",
    "\n",
    "# Check the test directory to see the newly created files\n",
    "# You can use the glob function to get a list of the file paths\n",
    "import glob\n",
    "file_paths = glob.glob(r'.\\test\\groupby-N_10000000_K_100_file_0.part*')\n",
    "\n",
    "# Create a Dask DataFrame from the multiple files\n",
    "df_dask = dd.read_csv(file_paths, dtype=dtypes)\n",
    "\n",
    "# Rerun the perform_test function on the updated Dask DataFrame\n",
    "execution_time = perform_test(df_dask)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 4a: parquet instead of csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dask.to_parquet('test/groupby.parquet', compression=None, engine='pyarrow')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
